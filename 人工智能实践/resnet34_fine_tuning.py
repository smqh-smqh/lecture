# -*- coding: utf-8 -*-
"""ResNet34_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bj6pxRNgD_Egz4o5HxtELldWUyK-AVca
"""

# !pip install tensorflow-gpu

import tensorflow as tf
import os
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense, \
    GlobalAveragePooling2D
from tensorflow.keras import Model
from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau
from matplotlib import pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.reshape(x_train.shape[0], 32, 32, 3)
x_test = x_test.reshape(x_test.shape[0], 32, 32, 3)
x_train, x_test = x_train / 255.0, x_test / 255.0
# x_train = tf.convert_to_tensor(x_train)
# x_test = tf.convert_to_tensor(x_test)

image_gen_train = ImageDataGenerator(
    rotation_range=10,  # 随机0度旋转
    width_shift_range=4,  # 宽度偏移
    height_shift_range=4,  # 高度偏移
    horizontal_flip=True,  # 水平翻转
)
image_gen_train.fit(x_train)

NUM_CLASSES = 10


class BasicBlock(Model):
    def __init__(self, filter_num, strides=1):
        super(BasicBlock, self).__init__()
        self.conv1 = Conv2D(filters=filter_num, kernel_size=(3, 3), strides=strides, padding='same', use_bias=False,
                            kernel_initializer=tf.random_normal_initializer())
        self.bn1 = BatchNormalization()
        self.conv2 = Conv2D(filters=filter_num, kernel_size=(3, 3), strides=1, padding='same', use_bias=False,
                            kernel_initializer=tf.random_normal_initializer())
        self.bn2 = BatchNormalization()

        if strides != 1:
            self.downsample = tf.keras.Sequential()
            self.downsample.add(Conv2D(filters=filter_num, kernel_size=(1, 1), strides=strides, padding='same',
                                       use_bias=False,
                                       kernel_initializer=tf.random_normal_initializer()))
            self.downsample.add(BatchNormalization())
        else:
            self.downsample = lambda x: x

    def call(self, inputs, training=None):
        identity = self.downsample(inputs)

        conv1 = self.conv1(inputs)
        bn1 = self.bn1(conv1, training=training)
        relu = tf.nn.relu(bn1)
        conv2 = self.conv2(relu)
        bn2 = self.bn2(conv2, training=training)

        output = tf.nn.relu(tf.keras.layers.add([identity, bn2]))

        return output


def build_res_block_1(filter_num, blocks, strides=1):
    res_block = tf.keras.Sequential()
    res_block.add(BasicBlock(filter_num, strides=strides))

    for _ in range(1, blocks):
        res_block.add(BasicBlock(filter_num, strides=1))

    return res_block


class ResNet34(Model):
    def __init__(self, num_classes=NUM_CLASSES):
        super(ResNet34, self).__init__()

        self.pre1 = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same')
        self.pre2 = BatchNormalization()
        self.pre3 = Activation('relu')
        self.pre4 = MaxPool2D(pool_size=(3, 3), strides=1)

        self.layer1 = build_res_block_1(filter_num=64, blocks=3)
        self.layer2 = build_res_block_1(filter_num=128, blocks=4, strides=2)
        self.layer3 = build_res_block_1(filter_num=256, blocks=6, strides=2)
        self.layer4 = build_res_block_1(filter_num=512, blocks=3, strides=2)

        self.avgpool = GlobalAveragePooling2D()
        self.fc = Dense(units=num_classes, activation=tf.keras.activations.softmax)

    def call(self, inputs, training=None, mask=None):
        pre1 = self.pre1(inputs)
        pre2 = self.pre2(pre1, training=training)
        pre3 = self.pre3(pre2)
        pre4 = self.pre4(pre3)
        l1 = self.layer1(pre4, training=training)
        l2 = self.layer2(l1, training=training)
        l3 = self.layer3(l2, training=training)
        l4 = self.layer4(l3, training=training)
        avgpool = self.avgpool(l4)
        out = self.fc(avgpool)

        return out


def lr_schedule(epoch):
    lr = 1e-3
    if epoch > 90:
        lr *= 1e-3
    elif epoch > 70:
        lr *= 1e-2
    elif epoch > 50:
        lr *= 1e-1
    print('Learning rate:', lr)
    return lr


model = ResNet34()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule(0)),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])

# model.compile(optimizer=tf.keras.optimizers.)

checkpoint_save_path = "./checkpoint_1/cifar10.ckpt"
if os.path.exists(checkpoint_save_path + '.index'):
    print('-------------load the model-----------------')
    model.load_weights(checkpoint_save_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                 save_weights_only=True,
                                                 monitor='val_sparse_categorical_accuracy',
                                                 save_best_only=True,
                                                 mode='max',
                                                 verbose=2)

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)

callbacks = [cp_callback, lr_scheduler, lr_reducer]

history = model.fit(image_gen_train.flow(x_train, y_train, batch_size=64), epochs=100,
                    validation_data=(x_test, y_test),
                    validation_freq=1, callbacks=callbacks, verbose=2)

model.summary()

# file = open('./weights.txt', 'w')  # 参数提取
# for v in model.trainable_variables:
#     file.write(str(v.name) + '\n')
#     file.write(str(v.shape) + '\n')
#     file.write(str(v.numpy()) + '\n')
# file.close()

###############################################    show   ###############################################

# 显示训练集和验证集的acc和loss曲线
acc = history.history['sparse_categorical_accuracy']
val_acc = history.history['val_sparse_categorical_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
# plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.show()

# plt.subplot(1, 2, 2)
plt.figure(figsize=(8, 8))
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
